{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Everbeek17/ML_RL_project/blob/main/colab_main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mN1355sQKOp"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mahNqO72QKOr"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_VCASN64QKOs"
      },
      "outputs": [],
      "source": [
        "# Import Gymnasium\n",
        "import gymnasium as gym\n",
        "\n",
        "# Import Wrappers\n",
        "from gymnasium.wrappers import GrayScaleObservation # Wrapper to convert RGB image to grayscale\n",
        "from stable_baselines3.common.vec_env import VecFrameStack, DummyVecEnv # VecFrameStack is a wrapper that stacks the last n frames\n",
        "\n",
        "# Import Algorithms for Training\n",
        "from stable_baselines3 import PPO # Proximal Policy Optimization (PPO) algorithm\n",
        "\n",
        "# Additional Imports\n",
        "import os # File management\n",
        "from stable_baselines3.common.callbacks import BaseCallback # Callbacks for saving models\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8paHRpJ6QKOu"
      },
      "source": [
        "### Functions and Definitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8gunO0MbQKOu"
      },
      "outputs": [],
      "source": [
        "CHECKPOINT_DIR = 'checkpoints' # Directory to save models\n",
        "LOG_DIR = 'logs'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TvvRFpp5QKOu"
      },
      "outputs": [],
      "source": [
        "# Custom class to make saving models easier\n",
        "class TrainAndLoggingCallback(BaseCallback):\n",
        "\n",
        "  def __init__(self, save_freq, save_path, save_prefix, verbose=1):\n",
        "    super(TrainAndLoggingCallback, self).__init__(verbose)\n",
        "    self.save_freq = save_freq\n",
        "    self.save_path = save_path\n",
        "    if save_prefix is not None:\n",
        "      self.save_prefix = save_prefix\n",
        "    else:\n",
        "      self.save_prefix = 'model'\n",
        "\n",
        "  def _init_callback(self):\n",
        "    if self.save_path is not None:\n",
        "      os.makedirs(self.save_path, exist_ok=True)\n",
        "\n",
        "  def _on_step(self):\n",
        "    if self.n_calls % self.save_freq == 0:\n",
        "      model_path = os.path.join(self.save_path, '{}_{}'.format(self.save_prefix, self.n_calls))\n",
        "      self.model.save(model_path)\n",
        "\n",
        "    return True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_LdDAfVWQKOv"
      },
      "outputs": [],
      "source": [
        "def initialize_env(game, frame_memory, render_mode = None):\n",
        "  # Create and define the environment\n",
        "  if (render_mode != None):\n",
        "    env = gym.make(game, render_mode = render_mode)\n",
        "  else:\n",
        "    env = gym.make(game)\n",
        "\n",
        "  env = GrayScaleObservation(env, keep_dim=True)                # Grayscale Wrapper to reduce the number of features\n",
        "  env = DummyVecEnv([lambda: env])                              # Wrapper to vectorize the environment (allows for multiple parallel environments)\n",
        "  env = VecFrameStack(env, frame_memory, channels_order='last') # FrameStack Wrapper to remember the last n frames\n",
        "\n",
        "  # Initializes the environment\n",
        "  env.reset()\n",
        "  return env\n",
        "\n",
        "def initialize_model(env, policy, learning_rate, n_steps):\n",
        "  # Initialize the model\n",
        "  model = PPO(policy, env, verbose=1, learning_rate=learning_rate, tensorboard_log=LOG_DIR, n_steps=n_steps)\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X0MDusHMQKOv"
      },
      "outputs": [],
      "source": [
        "def train_model(model_game,\n",
        "                model_policy,\n",
        "                model_learning_rate,\n",
        "                model_n_steps,\n",
        "                model_frame_memory=1,\n",
        "                training_length=1,\n",
        "                save_freq=1,\n",
        "                model_name=None,\n",
        "                pre_existing_model=None):\n",
        "  # Initialize the environment\n",
        "  env = initialize_env(model_game, model_frame_memory)\n",
        "\n",
        "  # Initialize the model\n",
        "  if pre_existing_model is None:\n",
        "    model = initialize_model(env, model_policy, model_learning_rate, model_n_steps)\n",
        "  else:\n",
        "    model = pre_existing_model\n",
        "\n",
        "  # Initialize \"callback\" to save the model\n",
        "  callback = TrainAndLoggingCallback(save_freq=save_freq, save_prefix=model_name, save_path=CHECKPOINT_DIR)\n",
        "\n",
        "  # Train the model for the specified number of timesteps\n",
        "  model.learn(total_timesteps=training_length, callback=callback)\n",
        "\n",
        "\n",
        "def load_model(game, frame_memory, full_model_name, render_mode = 'human'):\n",
        "  env = initialize_env(game, frame_memory, render_mode=render_mode)\n",
        "  model = PPO.load(os.path.join(CHECKPOINT_DIR, full_model_name))\n",
        "  return model, env\n",
        "\n",
        "def watch_model(model, env, num_actions):\n",
        "  state = env.reset()\n",
        "  for _ in range(num_actions):\n",
        "    action, _ = model.predict(state)\n",
        "    state, reward, done, info = env.step(action)\n",
        "    env.render()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kclIp6dhQKOw"
      },
      "source": [
        "## Start Doing Stuff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_rD78Le9QKOw"
      },
      "outputs": [],
      "source": [
        "full_model_name = \"TetrisPPO_mk1_1000000\"\n",
        "\n",
        "# Ideally these two are saved along with the model, and can be read in\n",
        "game = \"ALE/Tetris-v5\"\n",
        "frame_memory = 1\n",
        "\n",
        "# Also ideally all the rest of the model parameters are saved with the model\n",
        "\n",
        "model, env = load_model(game, frame_memory, full_model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "09fzSo8jQKOx"
      },
      "outputs": [],
      "source": [
        "watch_model(model, env, 2048)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rIKgsQA1QKOx"
      },
      "outputs": [],
      "source": [
        "# Train a new model\n",
        "model_name = \"SpaceInvadersPPO_mk2\"\n",
        "game = \"ALE/SpaceInvaders-v5\"\n",
        "frame_memory = 4\n",
        "policy = \"CnnPolicy\"\n",
        "learning_rate = 0.000001\n",
        "n_steps = 2048\n",
        "\n",
        "train_model(model_game=game,\n",
        "            model_policy=policy,\n",
        "            model_learning_rate=learning_rate,\n",
        "            model_n_steps=n_steps,\n",
        "            model_frame_memory=frame_memory,\n",
        "            training_length=2000000,\n",
        "            save_freq=500000,\n",
        "            model_name=model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QfDmTWtcQKOy"
      },
      "outputs": [],
      "source": [
        "# Train a new model\n",
        "model_name = \"TetrisPPO_mk1\"\n",
        "game = \"ALE/Tetris-v5\"\n",
        "frame_memory = 1\n",
        "policy = \"CnnPolicy\"\n",
        "learning_rate = 0.000001\n",
        "n_steps = 2048\n",
        "\n",
        "train_model(model_game=game,\n",
        "            model_policy=policy,\n",
        "            model_learning_rate=learning_rate,\n",
        "            model_n_steps=n_steps,\n",
        "            model_frame_memory=frame_memory,\n",
        "            training_length=1000000,\n",
        "            save_freq=250000,\n",
        "            model_name=model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "749OA4eyQKOy"
      },
      "source": [
        "# Recording Parameters for each model\n",
        "SpaceInvadersPPO_mk1\n",
        "- game = \"ALE/SpaceInvaders-v5\"\n",
        "- frame_memory = 4\n",
        "- policy = \"CnnPolicy\"\n",
        "- learning_rate = 0.000001\n",
        "- n_steps = 512\n",
        "\n",
        "\n",
        "\n",
        "SpaceInvadersPPO_mk2\n",
        "- game = \"ALE/SpaceInvaders-v5\"\n",
        "- frame_memory = 4\n",
        "- policy = \"CnnPolicy\"\n",
        "- learning_rate = 0.000001\n",
        "- n_steps = 2048\n",
        "\n",
        "\n",
        "SpaceInvadersPPO_mk2\n",
        "- game = \"ALE/SpaceInvaders-v5\"\n",
        "- frame_memory = 4\n",
        "- policy = \"CnnPolicy\"\n",
        "- learning_rate = 0.00001\n",
        "- n_steps = 2048\n",
        "\n",
        "TetrisPPO_mk1\n",
        "- game = \"ALE/Tetris-v5\"\n",
        "- frame_memory = 1\n",
        "- policy = \"CnnPolicy\"\n",
        "- learning_rate = 0.000001\n",
        "- n_steps = 2048\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "tensorboard --logdir=."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}